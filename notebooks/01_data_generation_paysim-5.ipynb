{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Generation for Spark Performance Benchmark (PaySim Dataset)\n",
    "\n",
    "**Objective:** Use the PaySim mobile money transaction dataset to benchmark I/O performance and join strategies.\n",
    "\n",
    "**Dataset:** PaySim1 - Mobile Money Transactions\n",
    "- Source: https://www.kaggle.com/datasets/ealaxi/paysim1\n",
    "- Size: ~6.3 Million transactions\n",
    "- Features: Transaction types, amounts, balances, fraud indicators\n",
    "\n",
    "This notebook:\n",
    "1. **Loads PaySim CSV data** from Kaggle\n",
    "2. **Creates two tables**:\n",
    "   - **fact_transactions**: Main transaction table (~6.3M rows)\n",
    "   - **dim_accounts**: Unique account dimension table\n",
    "3. **Saves in three formats**:\n",
    "   - **CSV**: Row-oriented, uncompressed\n",
    "   - **Parquet**: Columnar, compressed (Snappy)\n",
    "   - **Delta Lake**: Columnar, versioned, optimized with Z-Ordering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites: Download PaySim Dataset\n",
    "\n",
    "**Manual Download (Required):**\n",
    "1. Visit: https://www.kaggle.com/datasets/ealaxi/paysim1\n",
    "2. Click \"Download\" button (requires free Kaggle account)\n",
    "3. Extract the ZIP file\n",
    "4. Place `PS_20174392719_1491204439457_log.csv` in your project's `data/raw/` folder\n",
    "5. Optionally rename it to `paysim.csv` for simplicity\n",
    "\n",
    "**Expected file location:**\n",
    "```\n",
    "project/\n",
    "└── data/\n",
    "    └── raw/\n",
    "        └── paysim.csv  (or PS_20174392719_1491204439457_log.csv)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\samvo\\source\\repos\\Spark-Performance-Benchmark\n",
      "Src directory: C:\\Users\\samvo\\source\\repos\\Spark-Performance-Benchmark\\src\n"
     ]
    }
   ],
   "source": [
    "# Add src directory to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent\n",
    "src_dir = project_root / \"src\"\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Src directory: {src_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All directories verified/created successfully\n",
      "✓ HADOOP_HOME already set: C:\\hadoop\n"
     ]
    }
   ],
   "source": [
    "# Configure Hadoop for Windows (MUST run before creating SparkSession)\n",
    "from config import configure_hadoop_home\n",
    "configure_hadoop_home()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All directories verified/created successfully\n",
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pathlib import Path\n",
    "\n",
    "# Import project modules\n",
    "from config import (\n",
    "    get_data_path,\n",
    "    SPARK_APP_NAME,\n",
    "    ensure_directories_exist\n",
    ")\n",
    "from benchmark_utils import BenchmarkTimer, get_directory_size_mb\n",
    "\n",
    "# Ensure all directories exist\n",
    "ensure_directories_exist()\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark 3.5.0 session initialized with Delta Lake\n",
      "✓ Master: local[*]\n",
      "✓ App Name: SparkPerformanceBenchmark - PaySim Data Processing\n"
     ]
    }
   ],
   "source": [
    "# Create Spark session with Delta Lake support\n",
    "from delta import *\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder\n",
    "    .appName(f\"{SPARK_APP_NAME} - PaySim Data Processing\")\n",
    "    .master(\"local[*]\")  # Use all available cores\n",
    "    .config(\"spark.driver.memory\", \"4g\")  # Adjust based on available RAM\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\")  # Optimize for local execution\n",
    "    # --- WICHTIG: Delta Lake Konfigurationen ---\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    # Windows-Fix für temporäre Dateien:\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///C:/temp\") \n",
    ")\n",
    "\n",
    "# configure_spark_with_delta_pip ist der Schlüssel! Es verbindet dein pip-Paket mit Spark Java.\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(f\"✓ Spark {spark.version} session initialized with Delta Lake\")\n",
    "print(f\"✓ Master: {spark.sparkContext.master}\")\n",
    "print(f\"✓ App Name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Dataset and Display Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATASET FOUND\n",
      "======================================================================\n",
      "File: PS_20174392719_1491204439457_log.csv\n",
      "Path: C:\\Users\\samvo\\source\\repos\\Spark-Performance-Benchmark\\data\\raw\\PS_20174392719_1491204439457_log.csv\n",
      "Size: 470.67 MB\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Setup paths and check for PaySim CSV\n",
    "from pathlib import Path\n",
    "from config import RAW_DATA_DIR\n",
    "\n",
    "# Check for different possible filenames\n",
    "possible_files = [\n",
    "    RAW_DATA_DIR / \"paysim.csv\",\n",
    "    RAW_DATA_DIR / \"PS_20174392719_1491204439457_log.csv\"\n",
    "]\n",
    "\n",
    "PAYSIM_CSV_PATH = None\n",
    "for file_path in possible_files:\n",
    "    if file_path.exists():\n",
    "        PAYSIM_CSV_PATH = file_path\n",
    "        break\n",
    "\n",
    "if PAYSIM_CSV_PATH is None:\n",
    "    print(\"❌ ERROR: PaySim CSV file not found!\")\n",
    "    print(\"\\nPlease download the dataset from:\")\n",
    "    print(\"https://www.kaggle.com/datasets/ealaxi/paysim1\")\n",
    "    print(f\"\\nAnd place it in: {RAW_DATA_DIR}/\")\n",
    "    print(\"\\nAccepted filenames:\")\n",
    "    for f in possible_files:\n",
    "        print(f\"  - {f.name}\")\n",
    "    raise FileNotFoundError(\"PaySim CSV not found\")\n",
    "\n",
    "file_size_mb = PAYSIM_CSV_PATH.stat().st_size / (1024 * 1024)\n",
    "print(\"=\"*70)\n",
    "print(\"DATASET FOUND\")\n",
    "print(\"=\"*70)\n",
    "print(f\"File: {PAYSIM_CSV_PATH.name}\")\n",
    "print(f\"Path: {PAYSIM_CSV_PATH}\")\n",
    "print(f\"Size: {file_size_mb:.2f} MB\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PaySim Dataset\n",
    "\n",
    "**PaySim Schema:**\n",
    "- `step`: Time step (1 unit = 1 hour)\n",
    "- `type`: Transaction type (CASH-IN, CASH-OUT, DEBIT, PAYMENT, TRANSFER)\n",
    "- `amount`: Transaction amount\n",
    "- `nameOrig`: Customer who initiated the transaction\n",
    "- `oldbalanceOrg`: Initial balance before transaction\n",
    "- `newbalanceOrig`: New balance after transaction\n",
    "- `nameDest`: Customer recipient of transaction\n",
    "- `oldbalanceDest`: Initial recipient balance\n",
    "- `newbalanceDest`: New recipient balance\n",
    "- `isFraud`: 1 if fraud, 0 otherwise\n",
    "- `isFlaggedFraud`: 1 if flagged as fraud, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PaySim dataset...\n",
      "\n",
      "============================================================\n",
      "Starting benchmark: Load PaySim CSV\n",
      "Description: Reading 470.67 MB CSV file\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "✓ Completed: Load PaySim CSV\n",
      "Duration: 18.090 seconds (0.30 minutes)\n",
      "============================================================\n",
      "\n",
      "✓ Results logged to: C:\\Users\\samvo\\source\\repos\\Spark-Performance-Benchmark\\results\\benchmark_logs.csv\n",
      "✓ Loaded 6,362,620 transactions\n",
      "\n",
      "Sample data:\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|step|type    |amount  |nameOrig   |oldbalanceOrg|newbalanceOrig|nameDest   |oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|1   |PAYMENT |9839.64 |C1231006815|170136.0     |160296.36     |M1979787155|0.0           |0.0           |0      |0             |\n",
      "|1   |PAYMENT |1864.28 |C1666544295|21249.0      |19384.72      |M2044282225|0.0           |0.0           |0      |0             |\n",
      "|1   |TRANSFER|181.0   |C1305486145|181.0        |0.0           |C553264065 |0.0           |0.0           |1      |0             |\n",
      "|1   |CASH_OUT|181.0   |C840083671 |181.0        |0.0           |C38997010  |21182.0       |0.0           |1      |0             |\n",
      "|1   |PAYMENT |11668.14|C2048537720|41554.0      |29885.86      |M1230701703|0.0           |0.0           |0      |0             |\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load PaySim CSV\n",
    "print(\"Loading PaySim dataset...\")\n",
    "\n",
    "with BenchmarkTimer(\n",
    "    \"Load PaySim CSV\",\n",
    "    description=f\"Reading {file_size_mb:.2f} MB CSV file\",\n",
    "    spark=spark,\n",
    "    clear_cache=False\n",
    "):\n",
    "    paysim_raw = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .csv(str(PAYSIM_CSV_PATH))\n",
    "    )\n",
    "    \n",
    "    # Cache for reuse\n",
    "    paysim_raw.cache()\n",
    "    row_count = paysim_raw.count()\n",
    "\n",
    "print(f\"✓ Loaded {row_count:,} transactions\")\n",
    "print(\"\\nSample data:\")\n",
    "paysim_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: double (nullable = true)\n",
      " |-- newbalanceDest: double (nullable = true)\n",
      " |-- isFraud: integer (nullable = true)\n",
      " |-- isFlaggedFraud: integer (nullable = true)\n",
      "\n",
      "\n",
      "Data Quality Check:\n",
      "Total rows: 6,362,620\n",
      "Null values per column:\n",
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "|step|type|amount|nameOrig|oldbalanceOrg|newbalanceOrig|nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "|   0|   0|     0|       0|            0|             0|       0|             0|             0|      0|             0|\n",
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "\n",
      "\n",
      "Transaction Type Distribution:\n",
      "+--------+-------+\n",
      "|    type|  count|\n",
      "+--------+-------+\n",
      "|CASH_OUT|2237500|\n",
      "| PAYMENT|2151495|\n",
      "| CASH_IN|1399284|\n",
      "|TRANSFER| 532909|\n",
      "|   DEBIT|  41432|\n",
      "+--------+-------+\n",
      "\n",
      "\n",
      "Fraud Statistics:\n",
      "Fraudulent transactions: 8,213 (0.13%)\n",
      "Flagged transactions: 16\n"
     ]
    }
   ],
   "source": [
    "# Display schema and basic statistics\n",
    "print(\"Schema:\")\n",
    "paysim_raw.printSchema()\n",
    "\n",
    "print(\"\\nData Quality Check:\")\n",
    "print(f\"Total rows: {paysim_raw.count():,}\")\n",
    "print(f\"Null values per column:\")\n",
    "paysim_raw.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in paysim_raw.columns]).show()\n",
    "\n",
    "print(\"\\nTransaction Type Distribution:\")\n",
    "paysim_raw.groupBy(\"type\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "print(\"\\nFraud Statistics:\")\n",
    "fraud_stats = paysim_raw.agg(\n",
    "    F.sum(\"isFraud\").alias(\"total_fraud\"),\n",
    "    F.sum(\"isFlaggedFraud\").alias(\"total_flagged\"),\n",
    "    F.count(\"*\").alias(\"total_transactions\")\n",
    ").collect()[0]\n",
    "print(f\"Fraudulent transactions: {fraud_stats['total_fraud']:,} ({fraud_stats['total_fraud']/fraud_stats['total_transactions']*100:.2f}%)\")\n",
    "print(f\"Flagged transactions: {fraud_stats['total_flagged']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Fact Table: `fact_transactions`\n",
    "\n",
    "We'll use the PaySim data directly as our fact table with some transformations:\n",
    "- Rename columns for consistency\n",
    "- Add a proper transaction_id\n",
    "- Keep all relevant transaction details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fact_transactions table...\n",
      "✓ Created fact_transactions with 6,362,620 records\n",
      "\n",
      "Sample data:\n",
      "+--------------+---------+----------------+--------+------------+-------------------+------------------+------------+-------------------+------------------+--------+----------------+\n",
      "|transaction_id|time_step|transaction_type|amount  |account_orig|balance_orig_before|balance_orig_after|account_dest|balance_dest_before|balance_dest_after|is_fraud|is_flagged_fraud|\n",
      "+--------------+---------+----------------+--------+------------+-------------------+------------------+------------+-------------------+------------------+--------+----------------+\n",
      "|0             |1        |PAYMENT         |9839.64 |C1231006815 |170136.0           |160296.36         |M1979787155 |0.0                |0.0               |0       |0               |\n",
      "|1             |1        |PAYMENT         |1864.28 |C1666544295 |21249.0            |19384.72          |M2044282225 |0.0                |0.0               |0       |0               |\n",
      "|2             |1        |TRANSFER        |181.0   |C1305486145 |181.0              |0.0               |C553264065  |0.0                |0.0               |1       |0               |\n",
      "|3             |1        |CASH_OUT        |181.0   |C840083671  |181.0              |0.0               |C38997010   |21182.0            |0.0               |1       |0               |\n",
      "|4             |1        |PAYMENT         |11668.14|C2048537720 |41554.0            |29885.86          |M1230701703 |0.0                |0.0               |0       |0               |\n",
      "+--------------+---------+----------------+--------+------------+-------------------+------------------+------------+-------------------+------------------+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform PaySim data into fact_transactions\n",
    "print(\"Creating fact_transactions table...\")\n",
    "\n",
    "# Use monotonically_increasing_id() directly (no Window function needed!)\n",
    "# This is much faster as it avoids shuffling all data to a single partition\n",
    "fact_transactions = (\n",
    "    paysim_raw\n",
    "    .withColumn(\"transaction_id\", F.monotonically_increasing_id())\n",
    "    .select(\n",
    "        \"transaction_id\",\n",
    "        F.col(\"step\").alias(\"time_step\"),\n",
    "        F.col(\"type\").alias(\"transaction_type\"),\n",
    "        F.col(\"amount\").alias(\"amount\"),\n",
    "        F.col(\"nameOrig\").alias(\"account_orig\"),\n",
    "        F.col(\"oldbalanceOrg\").alias(\"balance_orig_before\"),\n",
    "        F.col(\"newbalanceOrig\").alias(\"balance_orig_after\"),\n",
    "        F.col(\"nameDest\").alias(\"account_dest\"),\n",
    "        F.col(\"oldbalanceDest\").alias(\"balance_dest_before\"),\n",
    "        F.col(\"newbalanceDest\").alias(\"balance_dest_after\"),\n",
    "        F.col(\"isFraud\").alias(\"is_fraud\"),\n",
    "        F.col(\"isFlaggedFraud\").alias(\"is_flagged_fraud\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cache for reuse during multiple writes\n",
    "fact_transactions.cache()\n",
    "fact_count = fact_transactions.count()\n",
    "\n",
    "print(f\"✓ Created fact_transactions with {fact_count:,} records\")\n",
    "print(\"\\nSample data:\")\n",
    "fact_transactions.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "root\n",
      " |-- transaction_id: long (nullable = false)\n",
      " |-- time_step: integer (nullable = true)\n",
      " |-- transaction_type: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- account_orig: string (nullable = true)\n",
      " |-- balance_orig_before: double (nullable = true)\n",
      " |-- balance_orig_after: double (nullable = true)\n",
      " |-- account_dest: string (nullable = true)\n",
      " |-- balance_dest_before: double (nullable = true)\n",
      " |-- balance_dest_after: double (nullable = true)\n",
      " |-- is_fraud: integer (nullable = true)\n",
      " |-- is_flagged_fraud: integer (nullable = true)\n",
      "\n",
      "\n",
      "Basic Statistics:\n",
      "+-------+------------------+-------------------+------------------+\n",
      "|summary|            amount|balance_orig_before|balance_orig_after|\n",
      "+-------+------------------+-------------------+------------------+\n",
      "|  count|           6362620|            6362620|           6362620|\n",
      "|   mean|179861.90354913156|  833883.1040744876| 855113.6685785913|\n",
      "| stddev| 603858.2314629381| 2888242.6730375625| 2924048.502954259|\n",
      "|    min|               0.0|                0.0|               0.0|\n",
      "|    max|     9.244551664E7|      5.958504037E7|     4.958504037E7|\n",
      "+-------+------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema and statistics\n",
    "print(\"Schema:\")\n",
    "fact_transactions.printSchema()\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "fact_transactions.select(\"amount\", \"balance_orig_before\", \"balance_orig_after\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dimension Table: `dim_accounts`\n",
    "\n",
    "Extract unique accounts from both origin and destination accounts:\n",
    "- Combine all unique account names\n",
    "- Calculate account statistics (total transactions, total volume, fraud rate)\n",
    "- Determine account type (Customer 'C' vs Merchant 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dim_accounts table...\n",
      "✓ Created dim_accounts with 9,073,900 unique accounts\n",
      "\n",
      "Sample data:\n",
      "+-----------+------------+------------------+------------+-----------+----------+\n",
      "|account_id |account_type|total_transactions|total_volume|fraud_count|fraud_rate|\n",
      "+-----------+------------+------------------+------------+-----------+----------+\n",
      "|C1000005555|Customer    |1                 |233109.79   |0          |0.0       |\n",
      "|C1000008393|Customer    |1                 |58347.84    |0          |0.0       |\n",
      "|C1000008582|Customer    |1                 |315626.96   |0          |0.0       |\n",
      "|C1000009272|Customer    |1                 |2262.44     |0          |0.0       |\n",
      "|C1000012233|Customer    |1                 |331041.93   |0          |0.0       |\n",
      "|C1000014489|Customer    |1                 |5787.18     |0          |0.0       |\n",
      "|C100002506 |Customer    |1                 |57887.52    |0          |0.0       |\n",
      "|C100002808 |Customer    |1                 |232765.42   |0          |0.0       |\n",
      "|C1000028463|Customer    |1                 |8916.05     |0          |0.0       |\n",
      "|C100002987 |Customer    |1                 |109756.08   |0          |0.0       |\n",
      "+-----------+------------+------------------+------------+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract unique accounts from origin accounts\n",
    "print(\"Creating dim_accounts table...\")\n",
    "\n",
    "# Get origin account statistics\n",
    "origin_stats = (\n",
    "    paysim_raw\n",
    "    .groupBy(F.col(\"nameOrig\").alias(\"account_id\"))\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_transactions_orig\"),\n",
    "        F.sum(\"amount\").alias(\"total_amount_orig\"),\n",
    "        F.sum(\"isFraud\").alias(\"fraud_count_orig\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Get destination account statistics\n",
    "dest_stats = (\n",
    "    paysim_raw\n",
    "    .groupBy(F.col(\"nameDest\").alias(\"account_id\"))\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_transactions_dest\"),\n",
    "        F.sum(\"amount\").alias(\"total_amount_dest\"),\n",
    "        F.sum(\"isFraud\").alias(\"fraud_count_dest\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Combine and create dimension table\n",
    "dim_accounts = (\n",
    "    origin_stats\n",
    "    .join(dest_stats, \"account_id\", \"full_outer\")\n",
    "    .fillna(0, subset=[\n",
    "        \"total_transactions_orig\", \"total_amount_orig\", \"fraud_count_orig\",\n",
    "        \"total_transactions_dest\", \"total_amount_dest\", \"fraud_count_dest\"\n",
    "    ])\n",
    "    .withColumn(\n",
    "        \"total_transactions\",\n",
    "        F.col(\"total_transactions_orig\") + F.col(\"total_transactions_dest\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"total_volume\",\n",
    "        F.col(\"total_amount_orig\") + F.col(\"total_amount_dest\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"fraud_count\",\n",
    "        F.col(\"fraud_count_orig\") + F.col(\"fraud_count_dest\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"account_type\",\n",
    "        F.when(F.col(\"account_id\").startswith(\"C\"), \"Customer\")\n",
    "         .when(F.col(\"account_id\").startswith(\"M\"), \"Merchant\")\n",
    "         .otherwise(\"Unknown\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"fraud_rate\",\n",
    "        F.when(F.col(\"total_transactions\") > 0, \n",
    "               F.col(\"fraud_count\") / F.col(\"total_transactions\"))\n",
    "         .otherwise(0.0)\n",
    "    )\n",
    "    .select(\n",
    "        \"account_id\",\n",
    "        \"account_type\",\n",
    "        \"total_transactions\",\n",
    "        \"total_volume\",\n",
    "        \"fraud_count\",\n",
    "        \"fraud_rate\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cache for reuse during multiple writes\n",
    "dim_accounts.cache()\n",
    "account_count = dim_accounts.count()\n",
    "\n",
    "print(f\"✓ Created dim_accounts with {account_count:,} unique accounts\")\n",
    "print(\"\\nSample data:\")\n",
    "dim_accounts.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "root\n",
      " |-- account_id: string (nullable = true)\n",
      " |-- account_type: string (nullable = false)\n",
      " |-- total_transactions: long (nullable = true)\n",
      " |-- total_volume: double (nullable = false)\n",
      " |-- fraud_count: long (nullable = true)\n",
      " |-- fraud_rate: double (nullable = true)\n",
      "\n",
      "\n",
      "Account Type Distribution:\n",
      "+------------+-------+\n",
      "|account_type|  count|\n",
      "+------------+-------+\n",
      "|    Customer|6923499|\n",
      "|    Merchant|2150401|\n",
      "+------------+-------+\n",
      "\n",
      "\n",
      "Top 10 Accounts by Transaction Volume:\n",
      "+-----------+------------+------------------+--------------------+-----------+--------------------+\n",
      "|account_id |account_type|total_transactions|total_volume        |fraud_count|fraud_rate          |\n",
      "+-----------+------------+------------------+--------------------+-----------+--------------------+\n",
      "|C439737079 |Customer    |18                |3.5744083144000006E8|0          |0.0                 |\n",
      "|C707403537 |Customer    |17                |2.993744184199999E8 |0          |0.0                 |\n",
      "|C167875008 |Customer    |28                |2.747364328E8       |0          |0.0                 |\n",
      "|C20253152  |Customer    |20                |2.7011618869E8      |0          |0.0                 |\n",
      "|C172409641 |Customer    |57                |2.5531017425000003E8|0          |0.0                 |\n",
      "|C268913927 |Customer    |35                |2.5348458809999996E8|0          |0.0                 |\n",
      "|C936857833 |Customer    |46                |2.2778001202E8      |0          |0.0                 |\n",
      "|C65111466  |Customer    |22                |2.2744384585000002E8|1          |0.045454545454545456|\n",
      "|C744189981 |Customer    |26                |2.2517386173000002E8|0          |0.0                 |\n",
      "|C1406193485|Customer    |46                |2.2477896182999998E8|0          |0.0                 |\n",
      "+-----------+------------+------------------+--------------------+-----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema and statistics\n",
    "print(\"Schema:\")\n",
    "dim_accounts.printSchema()\n",
    "\n",
    "print(\"\\nAccount Type Distribution:\")\n",
    "dim_accounts.groupBy(\"account_type\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "print(\"\\nTop 10 Accounts by Transaction Volume:\")\n",
    "dim_accounts.orderBy(\"total_volume\", ascending=False).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data in Multiple Formats\n",
    "\n",
    "We'll save both tables in three formats:\n",
    "1. **CSV** - Baseline format (row-oriented)\n",
    "2. **Parquet** - Columnar format with Snappy compression\n",
    "3. **Delta Lake** - Advanced format with ACID transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cache cleared for: Save fact_transactions as CSV\n",
      "\n",
      "============================================================\n",
      "Starting benchmark: Save fact_transactions as CSV\n",
      "Description: Writing 6,362,620 records to CSV\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "✓ Completed: Save fact_transactions as CSV\n",
      "Duration: 7.250 seconds (0.12 minutes)\n",
      "============================================================\n",
      "\n",
      "✓ Results logged to: C:\\Users\\samvo\\source\\repos\\Spark-Performance-Benchmark\\results\\benchmark_logs.csv\n",
      "CSV Size: 552.89 MB\n"
     ]
    }
   ],
   "source": [
    "# Save fact_transactions as CSV\n",
    "with BenchmarkTimer(\n",
    "    \"Save fact_transactions as CSV\",\n",
    "    description=f\"Writing {fact_count:,} records to CSV\",\n",
    "    spark=spark\n",
    "):\n",
    "    csv_path = str(get_data_path(\"csv\", \"fact_transactions\"))\n",
    "    fact_transactions.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_path)\n",
    "\n",
    "csv_size = get_directory_size_mb(get_data_path(\"csv\", \"fact_transactions\"))\n",
    "print(f\"CSV Size: {csv_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cache cleared for: Save dim_accounts as CSV\n",
      "\n",
      "============================================================\n",
      "Starting benchmark: Save dim_accounts as CSV\n",
      "Description: Writing 9,073,900 records to CSV\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "✓ Completed: Save dim_accounts as CSV\n",
      "Duration: 14.783 seconds (0.25 minutes)\n",
      "============================================================\n",
      "\n",
      "✓ Results logged to: C:\\Users\\samvo\\source\\repos\\Spark-Performance-Benchmark\\results\\benchmark_logs.csv\n",
      "CSV Size: 337.20 MB\n"
     ]
    }
   ],
   "source": [
    "# Save dim_accounts as CSV\n",
    "with BenchmarkTimer(\n",
    "    \"Save dim_accounts as CSV\",\n",
    "    description=f\"Writing {account_count:,} records to CSV\",\n",
    "    spark=spark\n",
    "):\n",
    "    csv_path = str(get_data_path(\"csv\", \"dim_accounts\"))\n",
    "    dim_accounts.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_path)\n",
    "\n",
    "csv_size = get_directory_size_mb(get_data_path(\"csv\", \"dim_accounts\"))\n",
    "print(f\"CSV Size: {csv_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as Parquet (with Snappy compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cache cleared for: Save fact_transactions as Parquet\n",
      "\n",
      "============================================================\n",
      "Starting benchmark: Save fact_transactions as Parquet\n",
      "Description: Writing 6,362,620 records to Parquet (Snappy)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "✓ Completed: Save fact_transactions as Parquet\n",
      "Duration: 8.856 seconds (0.15 minutes)\n",
      "============================================================\n",
      "\n",
      "✓ Results logged to: C:\\Users\\samvo\\source\\repos\\Spark-Performance-Benchmark\\results\\benchmark_logs.csv\n",
      "Parquet Size: 290.38 MB\n"
     ]
    }
   ],
   "source": [
    "# Save fact_transactions as Parquet\n",
    "with BenchmarkTimer(\n",
    "    \"Save fact_transactions as Parquet\",\n",
    "    description=f\"Writing {fact_count:,} records to Parquet (Snappy)\",\n",
    "    spark=spark\n",
    "):\n",
    "    parquet_path = str(get_data_path(\"parquet\", \"fact_transactions\"))\n",
    "    fact_transactions.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(parquet_path)\n",
    "\n",
    "parquet_size = get_directory_size_mb(get_data_path(\"parquet\", \"fact_transactions\"))\n",
    "print(f\"Parquet Size: {parquet_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cache cleared for: Save dim_accounts as Parquet\n",
      "\n",
      "============================================================\n",
      "Starting benchmark: Save dim_accounts as Parquet\n",
      "Description: Writing 9,073,900 records to Parquet (Snappy)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "✓ Completed: Save dim_accounts as Parquet\n",
      "Duration: 13.769 seconds (0.23 minutes)\n",
      "============================================================\n",
      "\n",
      "✓ Results logged to: C:\\Users\\samvo\\source\\repos\\Spark-Performance-Benchmark\\results\\benchmark_logs.csv\n",
      "Parquet Size: 112.84 MB\n"
     ]
    }
   ],
   "source": [
    "# Save dim_accounts as Parquet\n",
    "with BenchmarkTimer(\n",
    "    \"Save dim_accounts as Parquet\",\n",
    "    description=f\"Writing {account_count:,} records to Parquet (Snappy)\",\n",
    "    spark=spark\n",
    "):\n",
    "    parquet_path = str(get_data_path(\"parquet\", \"dim_accounts\"))\n",
    "    dim_accounts.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(parquet_path)\n",
    "\n",
    "parquet_size = get_directory_size_mb(get_data_path(\"parquet\", \"dim_accounts\"))\n",
    "print(f\"Parquet Size: {parquet_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as Delta Lake (with optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cache cleared for: Save fact_transactions as Delta\n",
      "\n",
      "============================================================\n",
      "Starting benchmark: Save fact_transactions as Delta\n",
      "Description: Writing 6,362,620 records to Delta Lake\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "✓ Completed: Save fact_transactions as Delta\n",
      "Duration: 11.348 seconds (0.19 minutes)\n",
      "============================================================\n",
      "\n",
      "✓ Results logged to: C:\\Users\\samvo\\source\\repos\\Spark-Performance-Benchmark\\results\\benchmark_logs.csv\n",
      "Delta Size (before optimization): 580.80 MB\n"
     ]
    }
   ],
   "source": [
    "# Save fact_transactions as Delta\n",
    "with BenchmarkTimer(\n",
    "    \"Save fact_transactions as Delta\",\n",
    "    description=f\"Writing {fact_count:,} records to Delta Lake\",\n",
    "    spark=spark\n",
    "):\n",
    "    delta_path = str(get_data_path(\"delta\", \"fact_transactions\"))\n",
    "    fact_transactions.write.mode(\"overwrite\").format(\"delta\").save(delta_path)\n",
    "\n",
    "delta_size_before = get_directory_size_mb(get_data_path(\"delta\", \"fact_transactions\"))\n",
    "print(f\"Delta Size (before optimization): {delta_size_before:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing fact_transactions Delta table with Z-ORDER BY account_orig...\n",
      "✓ Cache cleared for: Optimize fact_transactions Delta (ZORDER)\n",
      "\n",
      "============================================================\n",
      "Starting benchmark: Optimize fact_transactions Delta (ZORDER)\n",
      "Description: Running OPTIMIZE with ZORDER BY account_orig\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "✓ Completed: Optimize fact_transactions Delta (ZORDER)\n",
      "Duration: 21.682 seconds (0.36 minutes)\n",
      "============================================================\n",
      "\n",
      "✓ Results logged to: C:\\Users\\samvo\\source\\repos\\Spark-Performance-Benchmark\\results\\benchmark_logs.csv\n",
      "Delta Size (after optimization): 862.78 MB\n"
     ]
    }
   ],
   "source": [
    "# Optimize fact_transactions Delta table with Z-Ordering on account_orig\n",
    "# This improves join performance by co-locating related data\n",
    "print(\"Optimizing fact_transactions Delta table with Z-ORDER BY account_orig...\")\n",
    "\n",
    "with BenchmarkTimer(\n",
    "    \"Optimize fact_transactions Delta (ZORDER)\",\n",
    "    description=\"Running OPTIMIZE with ZORDER BY account_orig\",\n",
    "    spark=spark\n",
    "):\n",
    "    delta_path = str(get_data_path(\"delta\", \"fact_transactions\"))\n",
    "    spark.sql(f\"OPTIMIZE delta.`{delta_path}` ZORDER BY (account_orig)\")\n",
    "\n",
    "delta_size_after = get_directory_size_mb(get_data_path(\"delta\", \"fact_transactions\"))\n",
    "print(f\"Delta Size (after optimization): {delta_size_after:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cache cleared for: Save dim_accounts as Delta\n",
      "\n",
      "============================================================\n",
      "Starting benchmark: Save dim_accounts as Delta\n",
      "Description: Writing 9,073,900 records to Delta Lake\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "✓ Completed: Save dim_accounts as Delta\n",
      "Duration: 14.875 seconds (0.25 minutes)\n",
      "============================================================\n",
      "\n",
      "✓ Results logged to: C:\\Users\\samvo\\source\\repos\\Spark-Performance-Benchmark\\results\\benchmark_logs.csv\n",
      "Delta Size: 112.86 MB\n"
     ]
    }
   ],
   "source": [
    "# Save dim_accounts as Delta\n",
    "with BenchmarkTimer(\n",
    "    \"Save dim_accounts as Delta\",\n",
    "    description=f\"Writing {account_count:,} records to Delta Lake\",\n",
    "    spark=spark\n",
    "):\n",
    "    delta_path = str(get_data_path(\"delta\", \"dim_accounts\"))\n",
    "    dim_accounts.write.mode(\"overwrite\").format(\"delta\").save(delta_path)\n",
    "\n",
    "delta_size = get_directory_size_mb(get_data_path(\"delta\", \"dim_accounts\"))\n",
    "print(f\"Delta Size: {delta_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing dim_accounts Delta table...\n",
      "✓ Cache cleared for: Optimize dim_accounts Delta\n",
      "\n",
      "============================================================\n",
      "Starting benchmark: Optimize dim_accounts Delta\n",
      "Description: Running OPTIMIZE on dimension table\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "✓ Completed: Optimize dim_accounts Delta\n",
      "Duration: 10.378 seconds (0.17 minutes)\n",
      "============================================================\n",
      "\n",
      "✓ Results logged to: C:\\Users\\samvo\\source\\repos\\Spark-Performance-Benchmark\\results\\benchmark_logs.csv\n"
     ]
    }
   ],
   "source": [
    "# Optimize dim_accounts Delta table\n",
    "print(\"Optimizing dim_accounts Delta table...\")\n",
    "\n",
    "with BenchmarkTimer(\n",
    "    \"Optimize dim_accounts Delta\",\n",
    "    description=\"Running OPTIMIZE on dimension table\",\n",
    "    spark=spark\n",
    "):\n",
    "    delta_path = str(get_data_path(\"delta\", \"dim_accounts\"))\n",
    "    spark.sql(f\"OPTIMIZE delta.`{delta_path}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Storage Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "STORAGE COMPARISON SUMMARY\n",
      "==========================================================================================\n",
      " Format  fact_transactions (MB)  dim_accounts (MB)  Transactions Compression %  Accounts Compression %\n",
      "    CSV              552.894993         337.201606                         0.0                     0.0\n",
      "Parquet              290.375078         112.840783                        47.5                    66.5\n",
      "  Delta              862.776222         225.712968                       -56.0                    33.1\n",
      "==========================================================================================\n",
      "\n",
      "Note: Negative compression % means the format is larger than CSV\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display storage statistics\n",
    "import pandas as pd\n",
    "\n",
    "def get_format_sizes(table_name):\n",
    "    \"\"\"Get sizes for all formats of a table.\"\"\"\n",
    "    sizes = {}\n",
    "    for fmt in [\"csv\", \"parquet\", \"delta\"]:\n",
    "        path = get_data_path(fmt, table_name)\n",
    "        if path.exists():\n",
    "            sizes[fmt] = get_directory_size_mb(path)\n",
    "        else:\n",
    "            sizes[fmt] = 0.0\n",
    "    return sizes\n",
    "\n",
    "# Get sizes for both tables\n",
    "transactions_sizes = get_format_sizes(\"fact_transactions\")\n",
    "accounts_sizes = get_format_sizes(\"dim_accounts\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Format': ['CSV', 'Parquet', 'Delta'],\n",
    "    'fact_transactions (MB)': [transactions_sizes['csv'], transactions_sizes['parquet'], transactions_sizes['delta']],\n",
    "    'dim_accounts (MB)': [accounts_sizes['csv'], accounts_sizes['parquet'], accounts_sizes['delta']]\n",
    "})\n",
    "\n",
    "# Calculate compression ratios (vs CSV)\n",
    "comparison_df['Transactions Compression %'] = (\n",
    "    (1 - comparison_df['fact_transactions (MB)'] / transactions_sizes['csv']) * 100\n",
    ").round(1)\n",
    "comparison_df['Accounts Compression %'] = (\n",
    "    (1 - comparison_df['dim_accounts (MB)'] / accounts_sizes['csv']) * 100\n",
    ").round(1)\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"STORAGE COMPARISON SUMMARY\")\n",
    "print(\"=\"*90)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*90)\n",
    "print(f\"\\nNote: Negative compression % means the format is larger than CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification: Read Sample Data from Each Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from CSV:\n",
      "Count: 6,362,620\n",
      "+--------------+---------+----------------+---------+------------+-------------------+------------------+------------+-------------------+------------------+--------+----------------+\n",
      "|transaction_id|time_step|transaction_type|   amount|account_orig|balance_orig_before|balance_orig_after|account_dest|balance_dest_before|balance_dest_after|is_fraud|is_flagged_fraud|\n",
      "+--------------+---------+----------------+---------+------------+-------------------+------------------+------------+-------------------+------------------+--------+----------------+\n",
      "|  111669149696|      370|        CASH_OUT|131195.36|  C631630829|           43169.28|               0.0| C1324313732|          447142.99|         578338.34|       0|               0|\n",
      "|  111669149697|      370|         CASH_IN| 88693.24| C2094891829|           106252.0|         194945.24| C2051512830|      1.259020603E7|     1.250151279E7|       0|               0|\n",
      "|  111669149698|      370|         PAYMENT| 19514.83|  C136213783|          194945.24|         175430.41|  M540775640|                0.0|               0.0|       0|               0|\n",
      "+--------------+---------+----------------+---------+------------+-------------------+------------------+------------+-------------------+------------------+--------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify CSV\n",
    "print(\"Reading from CSV:\")\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\n",
    "    str(get_data_path(\"csv\", \"fact_transactions\"))\n",
    ")\n",
    "print(f\"Count: {csv_df.count():,}\")\n",
    "csv_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from Parquet:\n",
      "Count: 6,362,620\n",
      "+--------------+---------+----------------+---------+------------+-------------------+------------------+------------+-------------------+------------------+--------+----------------+\n",
      "|transaction_id|time_step|transaction_type|   amount|account_orig|balance_orig_before|balance_orig_after|account_dest|balance_dest_before|balance_dest_after|is_fraud|is_flagged_fraud|\n",
      "+--------------+---------+----------------+---------+------------+-------------------+------------------+------------+-------------------+------------------+--------+----------------+\n",
      "|    8589934592|       18|         PAYMENT| 12756.09| C1715701002|          171830.01|         159073.92|  M752830443|                0.0|               0.0|       0|               0|\n",
      "|    8589934593|       18|         PAYMENT|  8780.95| C1811023780|          159073.92|         150292.97| M1404577539|                0.0|               0.0|       0|               0|\n",
      "|    8589934594|       18|        TRANSFER|1233905.3|  C690590635|            70891.0|               0.0|   C79370952|          690564.69|        1924469.99|       0|               0|\n",
      "+--------------+---------+----------------+---------+------------+-------------------+------------------+------------+-------------------+------------------+--------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify Parquet\n",
    "print(\"Reading from Parquet:\")\n",
    "parquet_df = spark.read.parquet(str(get_data_path(\"parquet\", \"fact_transactions\")))\n",
    "print(f\"Count: {parquet_df.count():,}\")\n",
    "parquet_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from Delta:\n",
      "Count: 6,362,620\n",
      "+--------------+---------+----------------+---------+------------+-------------------+------------------+------------+-------------------+------------------+--------+----------------+\n",
      "|transaction_id|time_step|transaction_type|   amount|account_orig|balance_orig_before|balance_orig_after|account_dest|balance_dest_before|balance_dest_after|is_fraud|is_flagged_fraud|\n",
      "+--------------+---------+----------------+---------+------------+-------------------+------------------+------------+-------------------+------------------+--------+----------------+\n",
      "|    8589934592|       18|         PAYMENT| 12756.09| C1715701002|          171830.01|         159073.92|  M752830443|                0.0|               0.0|       0|               0|\n",
      "|    8589934593|       18|         PAYMENT|  8780.95| C1811023780|          159073.92|         150292.97| M1404577539|                0.0|               0.0|       0|               0|\n",
      "|    8589934594|       18|        TRANSFER|1233905.3|  C690590635|            70891.0|               0.0|   C79370952|          690564.69|        1924469.99|       0|               0|\n",
      "+--------------+---------+----------------+---------+------------+-------------------+------------------+------------+-------------------+------------------+--------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify Delta\n",
    "print(\"Reading from Delta:\")\n",
    "delta_df = spark.read.format(\"delta\").load(str(get_data_path(\"delta\", \"fact_transactions\")))\n",
    "print(f\"Count: {delta_df.count():,}\")\n",
    "delta_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cached DataFrames released\n",
      "\n",
      "================================================================================\n",
      "DATA PROCESSING COMPLETE (PaySim Dataset)\n",
      "================================================================================\n",
      "✓ Loaded PaySim dataset: 6,362,620 transactions\n",
      "✓ Created fact_transactions: 6,362,620 records\n",
      "✓ Created dim_accounts: 9,073,900 unique accounts\n",
      "✓ Saved in 3 formats: CSV, Parquet, Delta\n",
      "✓ Optimized Delta tables with Z-Ordering\n",
      "\n",
      "Next step: Run notebook 02_format_benchmark.ipynb\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Unpersist cached DataFrames\n",
    "paysim_raw.unpersist()\n",
    "fact_transactions.unpersist()\n",
    "dim_accounts.unpersist()\n",
    "\n",
    "print(\"✓ Cached DataFrames released\")\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA PROCESSING COMPLETE (PaySim Dataset)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Loaded PaySim dataset: {row_count:,} transactions\")\n",
    "print(f\"✓ Created fact_transactions: {fact_count:,} records\")\n",
    "print(f\"✓ Created dim_accounts: {account_count:,} unique accounts\")\n",
    "print(f\"✓ Saved in 3 formats: CSV, Parquet, Delta\")\n",
    "print(f\"✓ Optimized Delta tables with Z-Ordering\")\n",
    "print(f\"\\nNext step: Run notebook 02_format_benchmark.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
