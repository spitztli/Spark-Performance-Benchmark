{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Generation for Spark Performance Benchmark\n",
    "\n",
    "**Objective:** Generate synthetic datasets to benchmark I/O performance and join strategies.\n",
    "\n",
    "This notebook creates:\n",
    "1. **fact_sales**: Large fact table (~1-5M rows) with transaction data\n",
    "2. **dim_customers**: Dimension table (~100k rows) with customer information\n",
    "\n",
    "Data is saved in three formats:\n",
    "- **CSV**: Row-oriented, uncompressed\n",
    "- **Parquet**: Columnar, compressed (Snappy)\n",
    "- **Delta Lake**: Columnar, versioned, optimized with Z-Ordering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src directory to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent\n",
    "src_dir = project_root / \"src\"\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Src directory: {src_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Import project modules\n",
    "from config import (\n",
    "    get_data_path,\n",
    "    FACT_SALES_TABLE,\n",
    "    DIM_CUSTOMERS_TABLE,\n",
    "    SPARK_APP_NAME\n",
    ")\n",
    "from benchmark_utils import BenchmarkTimer, get_directory_size_mb\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with Delta Lake support\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(f\"{SPARK_APP_NAME} - Data Generation\")\n",
    "    .master(\"local[*]\")  # Use all available cores\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")  # Adjust based on available RAM\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\")  # Optimize for local execution\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"✓ Spark {spark.version} session initialized\")\n",
    "print(f\"✓ Master: {spark.sparkContext.master}\")\n",
    "print(f\"✓ App Name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration: Data Size Settings\n",
    "\n",
    "Adjust these parameters based on your system's capabilities:\n",
    "- **Small**: 100K sales, 10K customers (for testing)\n",
    "- **Medium**: 1M sales, 100K customers (recommended)\n",
    "- **Large**: 5M+ sales, 500K customers (for production benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Adjust based on your needs\n",
    "NUM_SALES_RECORDS = 2_000_000      # 2 Million sales transactions\n",
    "NUM_CUSTOMER_RECORDS = 100_000     # 100K customers\n",
    "\n",
    "# Seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Sales records: {NUM_SALES_RECORDS:,}\")\n",
    "print(f\"  - Customer records: {NUM_CUSTOMER_RECORDS:,}\")\n",
    "print(f\"  - Random seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Fact Table: `fact_sales`\n",
    "\n",
    "This represents a large transaction table with:\n",
    "- `transaction_id`: Unique identifier\n",
    "- `customer_id`: Foreign key to customers\n",
    "- `amount`: Transaction amount\n",
    "- `date`: Transaction date\n",
    "- `product_category`: Product category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fact_sales DataFrame\n",
    "print(\"Generating fact_sales table...\")\n",
    "\n",
    "# Product categories for variety\n",
    "categories = [\"Electronics\", \"Clothing\", \"Food\", \"Books\", \"Sports\", \"Home\", \"Beauty\", \"Toys\"]\n",
    "\n",
    "# Create the DataFrame using Spark functions\n",
    "fact_sales = (\n",
    "    spark.range(0, NUM_SALES_RECORDS)\n",
    "    .withColumn(\"transaction_id\", F.col(\"id\").cast(\"int\"))\n",
    "    .withColumn(\n",
    "        \"customer_id\",\n",
    "        (F.rand(seed=RANDOM_SEED) * NUM_CUSTOMER_RECORDS).cast(\"int\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"amount\",\n",
    "        (F.rand(seed=RANDOM_SEED + 1) * 1000 + 10).cast(\"double\")  # Between 10 and 1010\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"date\",\n",
    "        F.date_add(F.lit(\"2023-01-01\"), (F.rand(seed=RANDOM_SEED + 2) * 365).cast(\"int\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"product_category\",\n",
    "        F.array([F.lit(cat) for cat in categories]).getItem(\n",
    "            (F.rand(seed=RANDOM_SEED + 3) * len(categories)).cast(\"int\")\n",
    "        )\n",
    "    )\n",
    "    .drop(\"id\")\n",
    ")\n",
    "\n",
    "# Cache for reuse during multiple writes\n",
    "fact_sales.cache()\n",
    "fact_sales_count = fact_sales.count()\n",
    "\n",
    "print(f\"✓ Generated {fact_sales_count:,} sales records\")\n",
    "print(\"\\nSample data:\")\n",
    "fact_sales.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema and basic statistics\n",
    "print(\"Schema:\")\n",
    "fact_sales.printSchema()\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "fact_sales.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dimension Table: `dim_customers`\n",
    "\n",
    "This represents a smaller dimension table with:\n",
    "- `customer_id`: Primary key\n",
    "- `name`: Customer name\n",
    "- `region`: Geographic region\n",
    "- `signup_date`: Registration date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dim_customers DataFrame\n",
    "print(\"Generating dim_customers table...\")\n",
    "\n",
    "# Regions for diversity\n",
    "regions = [\"North\", \"South\", \"East\", \"West\", \"Central\"]\n",
    "\n",
    "dim_customers = (\n",
    "    spark.range(0, NUM_CUSTOMER_RECORDS)\n",
    "    .withColumn(\"customer_id\", F.col(\"id\").cast(\"int\"))\n",
    "    .withColumn(\n",
    "        \"name\",\n",
    "        F.concat(F.lit(\"Customer_\"), F.col(\"id\").cast(\"string\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"region\",\n",
    "        F.array([F.lit(reg) for reg in regions]).getItem(\n",
    "            (F.rand(seed=RANDOM_SEED + 4) * len(regions)).cast(\"int\")\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"signup_date\",\n",
    "        F.date_add(F.lit(\"2020-01-01\"), (F.rand(seed=RANDOM_SEED + 5) * 1095).cast(\"int\"))  # 3 years\n",
    "    )\n",
    "    .drop(\"id\")\n",
    ")\n",
    "\n",
    "# Cache for reuse during multiple writes\n",
    "dim_customers.cache()\n",
    "dim_customers_count = dim_customers.count()\n",
    "\n",
    "print(f\"✓ Generated {dim_customers_count:,} customer records\")\n",
    "print(\"\\nSample data:\")\n",
    "dim_customers.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema\n",
    "print(\"Schema:\")\n",
    "dim_customers.printSchema()\n",
    "\n",
    "print(\"\\nRegion Distribution:\")\n",
    "dim_customers.groupBy(\"region\").count().orderBy(\"region\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data in Multiple Formats\n",
    "\n",
    "We'll save both tables in three formats:\n",
    "1. **CSV** - Baseline format (row-oriented)\n",
    "2. **Parquet** - Columnar format with Snappy compression\n",
    "3. **Delta Lake** - Advanced format with ACID transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fact_sales as CSV\n",
    "with BenchmarkTimer(\n",
    "    \"Save fact_sales as CSV\",\n",
    "    description=f\"Writing {NUM_SALES_RECORDS:,} records to CSV\",\n",
    "    spark=spark\n",
    "):\n",
    "    csv_path = str(get_data_path(\"csv\", FACT_SALES_TABLE))\n",
    "    fact_sales.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_path)\n",
    "\n",
    "csv_size = get_directory_size_mb(get_data_path(\"csv\", FACT_SALES_TABLE))\n",
    "print(f\"CSV Size: {csv_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dim_customers as CSV\n",
    "with BenchmarkTimer(\n",
    "    \"Save dim_customers as CSV\",\n",
    "    description=f\"Writing {NUM_CUSTOMER_RECORDS:,} records to CSV\",\n",
    "    spark=spark\n",
    "):\n",
    "    csv_path = str(get_data_path(\"csv\", DIM_CUSTOMERS_TABLE))\n",
    "    dim_customers.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_path)\n",
    "\n",
    "csv_size = get_directory_size_mb(get_data_path(\"csv\", DIM_CUSTOMERS_TABLE))\n",
    "print(f\"CSV Size: {csv_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as Parquet (with Snappy compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fact_sales as Parquet\n",
    "with BenchmarkTimer(\n",
    "    \"Save fact_sales as Parquet\",\n",
    "    description=f\"Writing {NUM_SALES_RECORDS:,} records to Parquet (Snappy)\",\n",
    "    spark=spark\n",
    "):\n",
    "    parquet_path = str(get_data_path(\"parquet\", FACT_SALES_TABLE))\n",
    "    fact_sales.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(parquet_path)\n",
    "\n",
    "parquet_size = get_directory_size_mb(get_data_path(\"parquet\", FACT_SALES_TABLE))\n",
    "print(f\"Parquet Size: {parquet_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dim_customers as Parquet\n",
    "with BenchmarkTimer(\n",
    "    \"Save dim_customers as Parquet\",\n",
    "    description=f\"Writing {NUM_CUSTOMER_RECORDS:,} records to Parquet (Snappy)\",\n",
    "    spark=spark\n",
    "):\n",
    "    parquet_path = str(get_data_path(\"parquet\", DIM_CUSTOMERS_TABLE))\n",
    "    dim_customers.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(parquet_path)\n",
    "\n",
    "parquet_size = get_directory_size_mb(get_data_path(\"parquet\", DIM_CUSTOMERS_TABLE))\n",
    "print(f\"Parquet Size: {parquet_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as Delta Lake (with optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fact_sales as Delta\n",
    "with BenchmarkTimer(\n",
    "    \"Save fact_sales as Delta\",\n",
    "    description=f\"Writing {NUM_SALES_RECORDS:,} records to Delta Lake\",\n",
    "    spark=spark\n",
    "):\n",
    "    delta_path = str(get_data_path(\"delta\", FACT_SALES_TABLE))\n",
    "    fact_sales.write.mode(\"overwrite\").format(\"delta\").save(delta_path)\n",
    "\n",
    "delta_size_before = get_directory_size_mb(get_data_path(\"delta\", FACT_SALES_TABLE))\n",
    "print(f\"Delta Size (before optimization): {delta_size_before:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize fact_sales Delta table with Z-Ordering on customer_id\n",
    "# This improves join performance by co-locating related data\n",
    "print(\"Optimizing fact_sales Delta table with Z-ORDER BY customer_id...\")\n",
    "\n",
    "with BenchmarkTimer(\n",
    "    \"Optimize fact_sales Delta (ZORDER)\",\n",
    "    description=\"Running OPTIMIZE with ZORDER BY customer_id\",\n",
    "    spark=spark\n",
    "):\n",
    "    delta_path = str(get_data_path(\"delta\", FACT_SALES_TABLE))\n",
    "    spark.sql(f\"OPTIMIZE delta.`{delta_path}` ZORDER BY (customer_id)\")\n",
    "\n",
    "delta_size_after = get_directory_size_mb(get_data_path(\"delta\", FACT_SALES_TABLE))\n",
    "print(f\"Delta Size (after optimization): {delta_size_after:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dim_customers as Delta\n",
    "with BenchmarkTimer(\n",
    "    \"Save dim_customers as Delta\",\n",
    "    description=f\"Writing {NUM_CUSTOMER_RECORDS:,} records to Delta Lake\",\n",
    "    spark=spark\n",
    "):\n",
    "    delta_path = str(get_data_path(\"delta\", DIM_CUSTOMERS_TABLE))\n",
    "    dim_customers.write.mode(\"overwrite\").format(\"delta\").save(delta_path)\n",
    "\n",
    "delta_size = get_directory_size_mb(get_data_path(\"delta\", DIM_CUSTOMERS_TABLE))\n",
    "print(f\"Delta Size: {delta_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize dim_customers Delta table\n",
    "print(\"Optimizing dim_customers Delta table...\")\n",
    "\n",
    "with BenchmarkTimer(\n",
    "    \"Optimize dim_customers Delta\",\n",
    "    description=\"Running OPTIMIZE on dimension table\",\n",
    "    spark=spark\n",
    "):\n",
    "    delta_path = str(get_data_path(\"delta\", DIM_CUSTOMERS_TABLE))\n",
    "    spark.sql(f\"OPTIMIZE delta.`{delta_path}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Storage Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display storage statistics\n",
    "import pandas as pd\n",
    "\n",
    "def get_format_sizes(table_name):\n",
    "    \"\"\"Get sizes for all formats of a table.\"\"\"\n",
    "    sizes = {}\n",
    "    for fmt in [\"csv\", \"parquet\", \"delta\"]:\n",
    "        path = get_data_path(fmt, table_name)\n",
    "        if path.exists():\n",
    "            sizes[fmt] = get_directory_size_mb(path)\n",
    "        else:\n",
    "            sizes[fmt] = 0.0\n",
    "    return sizes\n",
    "\n",
    "# Get sizes for both tables\n",
    "sales_sizes = get_format_sizes(FACT_SALES_TABLE)\n",
    "customer_sizes = get_format_sizes(DIM_CUSTOMERS_TABLE)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Format': ['CSV', 'Parquet', 'Delta'],\n",
    "    'fact_sales (MB)': [sales_sizes['csv'], sales_sizes['parquet'], sales_sizes['delta']],\n",
    "    'dim_customers (MB)': [customer_sizes['csv'], customer_sizes['parquet'], customer_sizes['delta']]\n",
    "})\n",
    "\n",
    "# Calculate compression ratios (vs CSV)\n",
    "comparison_df['Sales Compression %'] = (\n",
    "    (1 - comparison_df['fact_sales (MB)'] / sales_sizes['csv']) * 100\n",
    ").round(1)\n",
    "comparison_df['Customer Compression %'] = (\n",
    "    (1 - comparison_df['dim_customers (MB)'] / customer_sizes['csv']) * 100\n",
    ").round(1)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STORAGE COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nNote: Negative compression % means the format is larger than CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification: Read Sample Data from Each Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify CSV\n",
    "print(\"Reading from CSV:\")\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\n",
    "    str(get_data_path(\"csv\", FACT_SALES_TABLE))\n",
    ")\n",
    "print(f\"Count: {csv_df.count():,}\")\n",
    "csv_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Parquet\n",
    "print(\"Reading from Parquet:\")\n",
    "parquet_df = spark.read.parquet(str(get_data_path(\"parquet\", FACT_SALES_TABLE)))\n",
    "print(f\"Count: {parquet_df.count():,}\")\n",
    "parquet_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Delta\n",
    "print(\"Reading from Delta:\")\n",
    "delta_df = spark.read.format(\"delta\").load(str(get_data_path(\"delta\", FACT_SALES_TABLE)))\n",
    "print(f\"Count: {delta_df.count():,}\")\n",
    "delta_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpersist cached DataFrames\n",
    "fact_sales.unpersist()\n",
    "dim_customers.unpersist()\n",
    "\n",
    "print(\"✓ Cached DataFrames released\")\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA GENERATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Generated {NUM_SALES_RECORDS:,} sales records\")\n",
    "print(f\"✓ Generated {NUM_CUSTOMER_RECORDS:,} customer records\")\n",
    "print(f\"✓ Saved in 3 formats: CSV, Parquet, Delta\")\n",
    "print(f\"✓ Optimized Delta tables with Z-Ordering\")\n",
    "print(f\"\\nNext step: Run notebook 02_format_benchmark.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
