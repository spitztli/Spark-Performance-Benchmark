{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Join Optimization: Sort-Merge vs. Broadcast Hash Join\n",
    "\n",
    "**Objective:** Compare performance of different join strategies in Spark.\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Sort-Merge Join**: Default join for large tables (requires shuffle)\n",
    "2. **Broadcast Hash Join**: Optimized join for small dimension tables\n",
    "3. **Impact of Z-Ordering**: Effect on join performance\n",
    "4. **Physical Plan Analysis**: Understanding Spark's execution plan\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src directory to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent\n",
    "src_dir = project_root / \"src\"\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Src directory: {src_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import project modules\n",
    "from config import (\n",
    "    get_data_path,\n",
    "    SPARK_APP_NAME,\n",
    "    PLOTS_DIR\n",
    ")\n",
    "from benchmark_utils import BenchmarkTimer, print_benchmark_summary\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"\u2713 All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Session\n",
    "\n",
    "We'll configure Spark with specific settings to control join behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with Delta Lake support\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(f\"{SPARK_APP_NAME} - Join Optimization\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "    # Configure broadcast join threshold (10MB default)\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")  # 10 MB\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Spark {spark.version} session initialized\")\n",
    "print(f\"\u2713 App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"\u2713 Broadcast Join Threshold: {spark.conf.get('spark.sql.autoBroadcastJoinThreshold')} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from Parquet Format\n",
    "\n",
    "We'll use Parquet format for these benchmarks to ensure optimal read performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fact_transactions from Parquet\n",
    "print(\"Loading fact_transactions table...\")\n",
    "fact_transactions_path = str(get_data_path(\"parquet\", \"fact_transactions\"))\n",
    "fact_transactions = spark.read.parquet(fact_transactions_path)\n",
    "print(f\"\u2713 Loaded {fact_transactions.count():,} sales records\")\n",
    "print(\"\\nSchema:\")\n",
    "fact_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dim_accounts from Parquet\n",
    "print(\"Loading dim_accounts table...\")\n",
    "dim_accounts_path = str(get_data_path(\"parquet\", \"dim_accounts\"))\n",
    "dim_accounts = spark.read.parquet(dim_accounts_path)\n",
    "print(f\"\u2713 Loaded {dim_accounts.count():,} customer records\")\n",
    "print(\"\\nSchema:\")\n",
    "dim_accounts.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Join Strategies\n",
    "\n",
    "### Sort-Merge Join\n",
    "- Both tables are sorted by join key\n",
    "- Requires shuffle (data movement across nodes)\n",
    "- Good for large-large table joins\n",
    "- More expensive due to shuffle overhead\n",
    "\n",
    "### Broadcast Hash Join\n",
    "- Small table is broadcasted to all nodes\n",
    "- No shuffle required\n",
    "- Excellent for large-small table joins (dimension tables)\n",
    "- Limited by broadcast threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario A: Sort-Merge Join (Without Optimization)\n",
    "\n",
    "First, we'll disable Adaptive Query Execution (AQE) to force a Sort-Merge Join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable Adaptive Query Execution for controlled testing\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "print(\"\u2713 AQE disabled for controlled join testing\")\n",
    "print(f\"\u2713 spark.sql.adaptive.enabled = {spark.conf.get('spark.sql.adaptive.enabled')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set broadcast threshold to -1 to disable automatic broadcast joins\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "print(\"\u2713 Broadcast joins disabled (threshold = -1)\")\n",
    "print(\"\u2713 This will force Sort-Merge Join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Sort-Merge Join\n",
    "with BenchmarkTimer(\n",
    "    \"Sort-Merge Join (Parquet)\",\n",
    "    description=\"Join fact_transactions with dim_accounts using Sort-Merge\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    "):\n",
    "    # Reload data to ensure clean state\n",
    "    sales_df = spark.read.parquet(fact_transactions_path)\n",
    "    customers_df = spark.read.parquet(dim_accounts_path)\n",
    "    \n",
    "    # Perform join\n",
    "    joined_df = sales_df.join(\n",
    "        customers_df,\n",
    "        on=sales_df[\"account_orig\"] == customers_df[\"account_id\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    # Trigger execution with an aggregation\n",
    "    result = joined_df.groupBy(\"account_type\").agg(\n",
    "        F.sum(\"amount\").alias(\"total_sales\"),\n",
    "        F.count(\"*\").alias(\"num_transactions\")\n",
    "    ).collect()\n",
    "    \n",
    "    print(f\"\\nRegions found: {len(result)}\")\n",
    "    for row in result:\n",
    "        print(f\"  {row['account_type']}: ${row['total_sales']:,.2f} ({row['num_transactions']:,} transactions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the physical plan to verify Sort-Merge Join was used\n",
    "print(\"\\nPhysical Plan (Sort-Merge Join):\")\n",
    "print(\"=\"*80)\n",
    "sales_df = spark.read.parquet(fact_transactions_path)\n",
    "customers_df = spark.read.parquet(dim_accounts_path)\n",
    "joined_df = sales_df.join(customers_df, on=sales_df[\"account_orig\"] == customers_df[\"account_id\"], how=\"inner\")\n",
    "joined_df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario B: Broadcast Hash Join (Optimized)\n",
    "\n",
    "Now we'll use the `broadcast()` hint to force a Broadcast Hash Join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-enable automatic broadcast joins\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")  # 10 MB\n",
    "print(\"\u2713 Broadcast joins re-enabled\")\n",
    "print(f\"\u2713 Threshold: {spark.conf.get('spark.sql.autoBroadcastJoinThreshold')} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Broadcast Hash Join with explicit broadcast hint\n",
    "with BenchmarkTimer(\n",
    "    \"Broadcast Hash Join (Parquet)\",\n",
    "    description=\"Join with broadcast hint on dim_accounts\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    "):\n",
    "    # Reload data to ensure clean state\n",
    "    sales_df = spark.read.parquet(fact_transactions_path)\n",
    "    customers_df = spark.read.parquet(dim_accounts_path)\n",
    "    \n",
    "    # Perform join with broadcast hint\n",
    "    joined_df = sales_df.join(\n",
    "        broadcast(customers_df),  # Broadcast the smaller dimension table\n",
    "        on=sales_df[\"account_orig\"] == customers_df[\"account_id\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    # Trigger execution with the same aggregation\n",
    "    result = joined_df.groupBy(\"account_type\").agg(\n",
    "        F.sum(\"amount\").alias(\"total_sales\"),\n",
    "        F.count(\"*\").alias(\"num_transactions\")\n",
    "    ).collect()\n",
    "    \n",
    "    print(f\"\\nRegions found: {len(result)}\")\n",
    "    for row in result:\n",
    "        print(f\"  {row['account_type']}: ${row['total_sales']:,.2f} ({row['num_transactions']:,} transactions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the physical plan to verify Broadcast Hash Join was used\n",
    "print(\"\\nPhysical Plan (Broadcast Hash Join):\")\n",
    "print(\"=\"*80)\n",
    "sales_df = spark.read.parquet(fact_transactions_path)\n",
    "customers_df = spark.read.parquet(dim_accounts_path)\n",
    "joined_df = sales_df.join(broadcast(customers_df), on=sales_df[\"account_orig\"] == customers_df[\"account_id\"], how=\"inner\")\n",
    "joined_df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario C: Join with Delta Lake (Z-Ordered)\n",
    "\n",
    "Test join performance with Delta Lake tables optimized with Z-Ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-enable AQE for Delta Lake testing\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")\n",
    "print(\"\u2713 AQE re-enabled for Delta Lake testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform join with Delta Lake (Z-Ordered on customer_id)\n",
    "with BenchmarkTimer(\n",
    "    \"Broadcast Hash Join (Delta Z-Ordered)\",\n",
    "    description=\"Join with Z-Ordered Delta table\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    "):\n",
    "    # Load from Delta Lake\n",
    "    delta_sales_path = str(get_data_path(\"delta\", \"fact_transactions\"))\n",
    "    delta_customers_path = str(get_data_path(\"delta\", \"dim_accounts\"))\n",
    "    \n",
    "    sales_df = spark.read.format(\"delta\").load(delta_sales_path)\n",
    "    customers_df = spark.read.format(\"delta\").load(delta_customers_path)\n",
    "    \n",
    "    # Perform join with broadcast hint\n",
    "    joined_df = sales_df.join(\n",
    "        broadcast(customers_df),\n",
    "        on=sales_df[\"account_orig\"] == customers_df[\"account_id\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    # Trigger execution\n",
    "    result = joined_df.groupBy(\"account_type\").agg(\n",
    "        F.sum(\"amount\").alias(\"total_sales\"),\n",
    "        F.count(\"*\").alias(\"num_transactions\")\n",
    "    ).collect()\n",
    "    \n",
    "    print(f\"\\nRegions found: {len(result)}\")\n",
    "    for row in result:\n",
    "        print(f\"  {row['account_type']}: ${row['total_sales']:,.2f} ({row['num_transactions']:,} transactions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario D: Sort-Merge Join with Delta (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable broadcast for Delta Sort-Merge comparison\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "\n",
    "with BenchmarkTimer(\n",
    "    \"Sort-Merge Join (Delta Z-Ordered)\",\n",
    "    description=\"Sort-Merge with Z-Ordered Delta table\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    "):\n",
    "    # Load from Delta Lake\n",
    "    delta_sales_path = str(get_data_path(\"delta\", \"fact_transactions\"))\n",
    "    delta_customers_path = str(get_data_path(\"delta\", \"dim_accounts\"))\n",
    "    \n",
    "    sales_df = spark.read.format(\"delta\").load(delta_sales_path)\n",
    "    customers_df = spark.read.format(\"delta\").load(delta_customers_path)\n",
    "    \n",
    "    # Perform join without broadcast hint\n",
    "    joined_df = sales_df.join(\n",
    "        customers_df,\n",
    "        on=sales_df[\"account_orig\"] == customers_df[\"account_id\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    # Trigger execution\n",
    "    result = joined_df.groupBy(\"account_type\").agg(\n",
    "        F.sum(\"amount\").alias(\"total_sales\"),\n",
    "        F.count(\"*\").alias(\"num_transactions\")\n",
    "    ).collect()\n",
    "    \n",
    "    print(f\"\\nRegions found: {len(result)}\")\n",
    "\n",
    "# Reset configurations\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell reads historical benchmark results from previous runs\n",
    "# On first run, you'll see 'No benchmark results found' - this is normal!\n",
    "# The benchmarks are still being executed and timed in the cells above.\n",
    "\n",
    "# Analyze benchmark results from log\n",
    "import csv\n",
    "import pandas as pd\n",
    "from config import BENCHMARK_LOG_FILE\n",
    "\n",
    "benchmark_results = []\n",
    "\n",
    "if BENCHMARK_LOG_FILE.exists():\n",
    "    with open(BENCHMARK_LOG_FILE, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            # Check if required keys exist\n",
    "            test_name = row.get('test_name', '')\n",
    "            status = row.get('status', '')\n",
    "            \n",
    "            if 'join' in test_name.lower() and status == 'SUCCESS':\n",
    "                benchmark_results.append(row)\n",
    "    \n",
    "    if benchmark_results:\n",
    "        # Create DataFrame for analysis\n",
    "        results_df = pd.DataFrame(benchmark_results)\n",
    "        results_df['duration_seconds'] = pd.to_numeric(results_df['duration_seconds'], errors='coerce')\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"JOIN BENCHMARK RESULTS SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(results_df[['test_name', 'duration_seconds', 'status']].to_string(index=False))\n",
    "        print(\"=\"*70)\n",
    "    else:\n",
    "        print(\"No benchmark results found for join operations\")\n",
    "else:\n",
    "    print(f\"Benchmark log file not found: {BENCHMARK_LOG_FILE}\")\n",
    "    print(\"Results have been timed but not logged to file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this analysis if we have benchmark results\n",
    "if 'results_df' in locals() and not results_df.empty:\n",
    "    # Visualize join performance comparison\n",
    "    recent_joins = results_df.tail(4).copy()\n",
    "    \n",
    "    if len(recent_joins) >= 4:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Shorten test names for display\n",
    "        labels = [\n",
    "            'Sort-Merge\\n(Parquet)',\n",
    "            'Broadcast\\n(Parquet)',\n",
    "            'Broadcast\\n(Delta Z-Order)',\n",
    "            'Sort-Merge\\n(Delta Z-Order)'\n",
    "        ]\n",
    "        \n",
    "        durations = recent_joins['duration_seconds'].values\n",
    "        colors = ['#e74c3c', '#2ecc71', '#3498db', '#f39c12']\n",
    "        \n",
    "        bars = ax.bar(labels, durations, color=colors, width=0.6)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, duration in zip(bars, durations):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{duration:.2f}s',\n",
    "                    ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        ax.set_title('Join Strategy Performance Comparison', fontsize=16, fontweight='bold')\n",
    "        ax.set_ylabel('Duration (seconds)', fontsize=12)\n",
    "        ax.set_xlabel('Join Strategy', fontsize=12)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Calculate and display speedup\n",
    "        if durations[0] > 0:\n",
    "            speedup = durations[0] / durations[1]\n",
    "            ax.text(0.98, 0.98, f'Broadcast Speedup: {speedup:.1f}x',\n",
    "                    transform=ax.transAxes, fontsize=12, verticalalignment='top',\n",
    "                    horizontalalignment='right', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(PLOTS_DIR / 'join_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\u2713 Plot saved to: {PLOTS_DIR / 'join_performance_comparison.png'}\")\n",
    "    else:\n",
    "        print(\"\u26a0 Not enough join benchmark data for visualization\")\n",
    "else:\n",
    "    print('Skipping visualization - no benchmark results available yet.')\n",
    "    print('Run the benchmark cells above first to generate data.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if we have join benchmark data\n",
    "if 'recent_joins' in locals() and len(recent_joins) > 0:\n",
    "    # Calculate performance improvements\n",
    "    if len(recent_joins) >= 2:\n",
    "        sort_merge_time = recent_joins.iloc[0]['duration_seconds']\n",
    "        broadcast_time = recent_joins.iloc[1]['duration_seconds']\n",
    "        \n",
    "        improvement = ((sort_merge_time - broadcast_time) / sort_merge_time) * 100\n",
    "        speedup = sort_merge_time / broadcast_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PERFORMANCE ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nSort-Merge Join Time:     {sort_merge_time:.3f} seconds\")\n",
    "        print(f\"Broadcast Hash Join Time: {broadcast_time:.3f} seconds\")\n",
    "        print(f\"\\nImprovement: {improvement:.1f}%\")\n",
    "        print(f\"Speedup:     {speedup:.2f}x faster\")\n",
    "        print(\"=\"*80)\n",
    "    else:\n",
    "        print(\"\u26a0 Not enough data for performance analysis\")",
    "else:\n",
    "    print('Skipping analysis - no join benchmark data available yet.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive summary\n",
    "try:\n",
    "    print_benchmark_summary()\n",
    "except Exception as e:\n",
    "    print(f'Note: Could not load benchmark summary: {e}')\n",
    "    print('This is normal on first run - results are still being logged.')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS - JOIN OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. BROADCAST HASH JOIN:\n",
    "   - Significantly faster than Sort-Merge Join (typically 2-5x)\n",
    "   - No shuffle required - dimension table sent to all nodes\n",
    "   - Ideal for large fact table + small dimension table (star schema)\n",
    "   - Limited by spark.sql.autoBroadcastJoinThreshold\n",
    "\n",
    "2. SORT-MERGE JOIN:\n",
    "   - Required when both tables are large\n",
    "   - Involves expensive shuffle operations\n",
    "   - Both sides must be sorted by join key\n",
    "   - Can benefit from partitioning and bucketing\n",
    "\n",
    "3. DELTA LAKE Z-ORDERING:\n",
    "   - Improves data locality for join keys\n",
    "   - Can reduce I/O by co-locating related data\n",
    "   - Especially beneficial for Sort-Merge Joins\n",
    "   - Combines well with data skipping\n",
    "\n",
    "4. ADAPTIVE QUERY EXECUTION (AQE):\n",
    "   - Dynamically optimizes joins at runtime\n",
    "   - Can convert Sort-Merge to Broadcast automatically\n",
    "   - Reduces shuffle overhead when possible\n",
    "\n",
    "5. BEST PRACTICES:\n",
    "   - Use broadcast() hint for dimension tables < 100MB\n",
    "   - Consider partitioning large fact tables by join keys\n",
    "   - Enable AQE in production (spark.sql.adaptive.enabled=true)\n",
    "   - Monitor Spark UI to verify join strategy selection\n",
    "   - Use Z-Ordering on frequently joined columns in Delta Lake\n",
    "\"\"\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\u2713 Join optimization benchmark completed!\")\n",
    "print(\"\u2713 All benchmarks finished. Check results/ directory for logs and plots.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}