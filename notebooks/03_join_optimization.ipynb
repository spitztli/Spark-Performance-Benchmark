{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Join Optimization: Sort-Merge vs. Broadcast Hash Join\n",
    "\n",
    "**Objective:** Compare performance of different join strategies in Spark.\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Sort-Merge Join**: Default join for large tables (requires shuffle)\n",
    "2. **Broadcast Hash Join**: Optimized join for small dimension tables\n",
    "3. **Impact of Z-Ordering**: Effect on join performance\n",
    "4. **Physical Plan Analysis**: Understanding Spark's execution plan\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src directory to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent\n",
    "src_dir = project_root / \"src\"\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Src directory: {src_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import project modules\n",
    "from config import (\n",
    "    get_data_path,\n",
    "    FACT_SALES_TABLE,\n",
    "    DIM_CUSTOMERS_TABLE,\n",
    "    SPARK_APP_NAME,\n",
    "    PLOTS_DIR\n",
    ")\n",
    "from benchmark_utils import BenchmarkTimer, print_benchmark_summary\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Session\n",
    "\n",
    "We'll configure Spark with specific settings to control join behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with Delta Lake support\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(f\"{SPARK_APP_NAME} - Join Optimization\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "    # Configure broadcast join threshold (10MB default)\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")  # 10 MB\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"✓ Spark {spark.version} session initialized\")\n",
    "print(f\"✓ App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"✓ Broadcast Join Threshold: {spark.conf.get('spark.sql.autoBroadcastJoinThreshold')} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from Parquet Format\n",
    "\n",
    "We'll use Parquet format for these benchmarks to ensure optimal read performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fact_sales from Parquet\n",
    "print(\"Loading fact_sales table...\")\n",
    "fact_sales_path = str(get_data_path(\"parquet\", FACT_SALES_TABLE))\n",
    "fact_sales = spark.read.parquet(fact_sales_path)\n",
    "print(f\"✓ Loaded {fact_sales.count():,} sales records\")\n",
    "print(\"\\nSchema:\")\n",
    "fact_sales.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dim_customers from Parquet\n",
    "print(\"Loading dim_customers table...\")\n",
    "dim_customers_path = str(get_data_path(\"parquet\", DIM_CUSTOMERS_TABLE))\n",
    "dim_customers = spark.read.parquet(dim_customers_path)\n",
    "print(f\"✓ Loaded {dim_customers.count():,} customer records\")\n",
    "print(\"\\nSchema:\")\n",
    "dim_customers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Join Strategies\n",
    "\n",
    "### Sort-Merge Join\n",
    "- Both tables are sorted by join key\n",
    "- Requires shuffle (data movement across nodes)\n",
    "- Good for large-large table joins\n",
    "- More expensive due to shuffle overhead\n",
    "\n",
    "### Broadcast Hash Join\n",
    "- Small table is broadcasted to all nodes\n",
    "- No shuffle required\n",
    "- Excellent for large-small table joins (dimension tables)\n",
    "- Limited by broadcast threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario A: Sort-Merge Join (Without Optimization)\n",
    "\n",
    "First, we'll disable Adaptive Query Execution (AQE) to force a Sort-Merge Join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable Adaptive Query Execution for controlled testing\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "print(\"✓ AQE disabled for controlled join testing\")\n",
    "print(f\"✓ spark.sql.adaptive.enabled = {spark.conf.get('spark.sql.adaptive.enabled')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set broadcast threshold to -1 to disable automatic broadcast joins\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "print(\"✓ Broadcast joins disabled (threshold = -1)\")\n",
    "print(\"✓ This will force Sort-Merge Join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Sort-Merge Join\n",
    "with BenchmarkTimer(\n",
    "    \"Sort-Merge Join (Parquet)\",\n",
    "    description=\"Join fact_sales with dim_customers using Sort-Merge\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    "):\n",
    "    # Reload data to ensure clean state\n",
    "    sales_df = spark.read.parquet(fact_sales_path)\n",
    "    customers_df = spark.read.parquet(dim_customers_path)\n",
    "    \n",
    "    # Perform join\n",
    "    joined_df = sales_df.join(\n",
    "        customers_df,\n",
    "        on=\"customer_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    # Trigger execution with an aggregation\n",
    "    result = joined_df.groupBy(\"region\").agg(\n",
    "        F.sum(\"amount\").alias(\"total_sales\"),\n",
    "        F.count(\"*\").alias(\"num_transactions\")\n",
    "    ).collect()\n",
    "    \n",
    "    print(f\"\\nRegions found: {len(result)}\")\n",
    "    for row in result:\n",
    "        print(f\"  {row['region']}: ${row['total_sales']:,.2f} ({row['num_transactions']:,} transactions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the physical plan to verify Sort-Merge Join was used\n",
    "print(\"\\nPhysical Plan (Sort-Merge Join):\")\n",
    "print(\"=\"*80)\n",
    "sales_df = spark.read.parquet(fact_sales_path)\n",
    "customers_df = spark.read.parquet(dim_customers_path)\n",
    "joined_df = sales_df.join(customers_df, on=\"customer_id\", how=\"inner\")\n",
    "joined_df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario B: Broadcast Hash Join (Optimized)\n",
    "\n",
    "Now we'll use the `broadcast()` hint to force a Broadcast Hash Join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-enable automatic broadcast joins\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")  # 10 MB\n",
    "print(\"✓ Broadcast joins re-enabled\")\n",
    "print(f\"✓ Threshold: {spark.conf.get('spark.sql.autoBroadcastJoinThreshold')} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Broadcast Hash Join with explicit broadcast hint\n",
    "with BenchmarkTimer(\n",
    "    \"Broadcast Hash Join (Parquet)\",\n",
    "    description=\"Join with broadcast hint on dim_customers\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    "):\n",
    "    # Reload data to ensure clean state\n",
    "    sales_df = spark.read.parquet(fact_sales_path)\n",
    "    customers_df = spark.read.parquet(dim_customers_path)\n",
    "    \n",
    "    # Perform join with broadcast hint\n",
    "    joined_df = sales_df.join(\n",
    "        broadcast(customers_df),  # Broadcast the smaller dimension table\n",
    "        on=\"customer_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    # Trigger execution with the same aggregation\n",
    "    result = joined_df.groupBy(\"region\").agg(\n",
    "        F.sum(\"amount\").alias(\"total_sales\"),\n",
    "        F.count(\"*\").alias(\"num_transactions\")\n",
    "    ).collect()\n",
    "    \n",
    "    print(f\"\\nRegions found: {len(result)}\")\n",
    "    for row in result:\n",
    "        print(f\"  {row['region']}: ${row['total_sales']:,.2f} ({row['num_transactions']:,} transactions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the physical plan to verify Broadcast Hash Join was used\n",
    "print(\"\\nPhysical Plan (Broadcast Hash Join):\")\n",
    "print(\"=\"*80)\n",
    "sales_df = spark.read.parquet(fact_sales_path)\n",
    "customers_df = spark.read.parquet(dim_customers_path)\n",
    "joined_df = sales_df.join(broadcast(customers_df), on=\"customer_id\", how=\"inner\")\n",
    "joined_df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario C: Join with Delta Lake (Z-Ordered)\n",
    "\n",
    "Test join performance with Delta Lake tables optimized with Z-Ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-enable AQE for Delta Lake testing\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")\n",
    "print(\"✓ AQE re-enabled for Delta Lake testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform join with Delta Lake (Z-Ordered on customer_id)\n",
    "with BenchmarkTimer(\n",
    "    \"Broadcast Hash Join (Delta Z-Ordered)\",\n",
    "    description=\"Join with Z-Ordered Delta table\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    "):\n",
    "    # Load from Delta Lake\n",
    "    delta_sales_path = str(get_data_path(\"delta\", FACT_SALES_TABLE))\n",
    "    delta_customers_path = str(get_data_path(\"delta\", DIM_CUSTOMERS_TABLE))\n",
    "    \n",
    "    sales_df = spark.read.format(\"delta\").load(delta_sales_path)\n",
    "    customers_df = spark.read.format(\"delta\").load(delta_customers_path)\n",
    "    \n",
    "    # Perform join with broadcast hint\n",
    "    joined_df = sales_df.join(\n",
    "        broadcast(customers_df),\n",
    "        on=\"customer_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    # Trigger execution\n",
    "    result = joined_df.groupBy(\"region\").agg(\n",
    "        F.sum(\"amount\").alias(\"total_sales\"),\n",
    "        F.count(\"*\").alias(\"num_transactions\")\n",
    "    ).collect()\n",
    "    \n",
    "    print(f\"\\nRegions found: {len(result)}\")\n",
    "    for row in result:\n",
    "        print(f\"  {row['region']}: ${row['total_sales']:,.2f} ({row['num_transactions']:,} transactions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario D: Sort-Merge Join with Delta (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable broadcast for Delta Sort-Merge comparison\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "\n",
    "with BenchmarkTimer(\n",
    "    \"Sort-Merge Join (Delta Z-Ordered)\",\n",
    "    description=\"Sort-Merge with Z-Ordered Delta table\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    "):\n",
    "    # Load from Delta Lake\n",
    "    delta_sales_path = str(get_data_path(\"delta\", FACT_SALES_TABLE))\n",
    "    delta_customers_path = str(get_data_path(\"delta\", DIM_CUSTOMERS_TABLE))\n",
    "    \n",
    "    sales_df = spark.read.format(\"delta\").load(delta_sales_path)\n",
    "    customers_df = spark.read.format(\"delta\").load(delta_customers_path)\n",
    "    \n",
    "    # Perform join without broadcast hint\n",
    "    joined_df = sales_df.join(\n",
    "        customers_df,\n",
    "        on=\"customer_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    # Trigger execution\n",
    "    result = joined_df.groupBy(\"region\").agg(\n",
    "        F.sum(\"amount\").alias(\"total_sales\"),\n",
    "        F.count(\"*\").alias(\"num_transactions\")\n",
    "    ).collect()\n",
    "    \n",
    "    print(f\"\\nRegions found: {len(result)}\")\n",
    "\n",
    "# Reset configurations\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmark results for join tests\n",
    "from config import BENCHMARK_LOG_FILE\n",
    "import csv\n",
    "\n",
    "join_results = []\n",
    "with open(BENCHMARK_LOG_FILE, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        if 'Join' in row['test_name'] and row['status'] == 'SUCCESS':\n",
    "            join_results.append(row)\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "results_df = pd.DataFrame(join_results)\n",
    "results_df['duration_seconds'] = results_df['duration_seconds'].astype(float)\n",
    "\n",
    "# Display recent join benchmarks\n",
    "print(\"\\nJoin Benchmark Results:\")\n",
    "print(results_df[['test_name', 'duration_seconds']].tail(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize join performance comparison\n",
    "recent_joins = results_df.tail(4).copy()\n",
    "\n",
    "if len(recent_joins) >= 4:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Shorten test names for display\n",
    "    labels = [\n",
    "        'Sort-Merge\\n(Parquet)',\n",
    "        'Broadcast\\n(Parquet)',\n",
    "        'Broadcast\\n(Delta Z-Order)',\n",
    "        'Sort-Merge\\n(Delta Z-Order)'\n",
    "    ]\n",
    "    \n",
    "    durations = recent_joins['duration_seconds'].values\n",
    "    colors = ['#e74c3c', '#2ecc71', '#3498db', '#f39c12']\n",
    "    \n",
    "    bars = ax.bar(labels, durations, color=colors, width=0.6)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, duration in zip(bars, durations):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{duration:.2f}s',\n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    ax.set_title('Join Strategy Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylabel('Duration (seconds)', fontsize=12)\n",
    "    ax.set_xlabel('Join Strategy', fontsize=12)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Calculate and display speedup\n",
    "    if durations[0] > 0:\n",
    "        speedup = durations[0] / durations[1]\n",
    "        ax.text(0.98, 0.98, f'Broadcast Speedup: {speedup:.1f}x',\n",
    "                transform=ax.transAxes, fontsize=12, verticalalignment='top',\n",
    "                horizontalalignment='right', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / 'join_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Plot saved to: {PLOTS_DIR / 'join_performance_comparison.png'}\")\n",
    "else:\n",
    "    print(\"⚠ Not enough join benchmark data for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance improvements\n",
    "if len(recent_joins) >= 2:\n",
    "    sort_merge_time = recent_joins.iloc[0]['duration_seconds']\n",
    "    broadcast_time = recent_joins.iloc[1]['duration_seconds']\n",
    "    \n",
    "    improvement = ((sort_merge_time - broadcast_time) / sort_merge_time) * 100\n",
    "    speedup = sort_merge_time / broadcast_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nSort-Merge Join Time:     {sort_merge_time:.3f} seconds\")\n",
    "    print(f\"Broadcast Hash Join Time: {broadcast_time:.3f} seconds\")\n",
    "    print(f\"\\nImprovement: {improvement:.1f}%\")\n",
    "    print(f\"Speedup:     {speedup:.2f}x faster\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"⚠ Not enough data for performance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive summary\n",
    "print_benchmark_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS - JOIN OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. BROADCAST HASH JOIN:\n",
    "   - Significantly faster than Sort-Merge Join (typically 2-5x)\n",
    "   - No shuffle required - dimension table sent to all nodes\n",
    "   - Ideal for large fact table + small dimension table (star schema)\n",
    "   - Limited by spark.sql.autoBroadcastJoinThreshold\n",
    "\n",
    "2. SORT-MERGE JOIN:\n",
    "   - Required when both tables are large\n",
    "   - Involves expensive shuffle operations\n",
    "   - Both sides must be sorted by join key\n",
    "   - Can benefit from partitioning and bucketing\n",
    "\n",
    "3. DELTA LAKE Z-ORDERING:\n",
    "   - Improves data locality for join keys\n",
    "   - Can reduce I/O by co-locating related data\n",
    "   - Especially beneficial for Sort-Merge Joins\n",
    "   - Combines well with data skipping\n",
    "\n",
    "4. ADAPTIVE QUERY EXECUTION (AQE):\n",
    "   - Dynamically optimizes joins at runtime\n",
    "   - Can convert Sort-Merge to Broadcast automatically\n",
    "   - Reduces shuffle overhead when possible\n",
    "\n",
    "5. BEST PRACTICES:\n",
    "   - Use broadcast() hint for dimension tables < 100MB\n",
    "   - Consider partitioning large fact tables by join keys\n",
    "   - Enable AQE in production (spark.sql.adaptive.enabled=true)\n",
    "   - Monitor Spark UI to verify join strategy selection\n",
    "   - Use Z-Ordering on frequently joined columns in Delta Lake\n",
    "\"\"\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n✓ Join optimization benchmark completed!\")\n",
    "print(\"✓ All benchmarks finished. Check results/ directory for logs and plots.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
