{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Format Benchmark: CSV vs. Parquet vs. Delta Lake\n",
    "\n",
    "**Objective:** Compare I/O performance and storage efficiency across different file formats.\n",
    "\n",
    "This notebook measures:\n",
    "1. **Read Performance**: Time to load data from each format\n",
    "2. **Storage Efficiency**: Disk space used by each format\n",
    "3. **Query Performance**: Aggregation speed (columnar vs. row-oriented)\n",
    "4. **Filter Pushdown**: Predicate pushdown effectiveness\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src directory to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent\n",
    "src_dir = project_root / \"src\"\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Src directory: {src_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import project modules\n",
    "from config import (\n",
    "    get_data_path,\n",
    "    FACT_SALES_TABLE,\n",
    "    DIM_CUSTOMERS_TABLE,\n",
    "    SPARK_APP_NAME,\n",
    "    PLOTS_DIR\n",
    ")\n",
    "from benchmark_utils import BenchmarkTimer, get_directory_size_mb, print_benchmark_summary\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with Delta Lake support\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(f\"{SPARK_APP_NAME} - Format Benchmark\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"✓ Spark {spark.version} session initialized\")\n",
    "print(f\"✓ App Name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage Size Comparison\n",
    "\n",
    "First, let's compare the disk space used by each format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate storage sizes for all formats\n",
    "storage_data = []\n",
    "\n",
    "for table_name in [FACT_SALES_TABLE, DIM_CUSTOMERS_TABLE]:\n",
    "    for fmt in ['csv', 'parquet', 'delta']:\n",
    "        path = get_data_path(fmt, table_name)\n",
    "        if path.exists():\n",
    "            size_mb = get_directory_size_mb(path)\n",
    "            storage_data.append({\n",
    "                'Table': table_name,\n",
    "                'Format': fmt.upper(),\n",
    "                'Size_MB': size_mb\n",
    "            })\n",
    "\n",
    "# Create DataFrame\n",
    "storage_df = pd.DataFrame(storage_data)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*60)\n",
    "print(\"STORAGE SIZE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(storage_df.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize storage comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot for fact_sales\n",
    "sales_data = storage_df[storage_df['Table'] == FACT_SALES_TABLE]\n",
    "axes[0].bar(sales_data['Format'], sales_data['Size_MB'], color=['#e74c3c', '#3498db', '#2ecc71'])\n",
    "axes[0].set_title('fact_sales - Storage Size Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Size (MB)', fontsize=12)\n",
    "axes[0].set_xlabel('Format', fontsize=12)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(sales_data['Size_MB']):\n",
    "    axes[0].text(i, v + 5, f'{v:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot for dim_customers\n",
    "customer_data = storage_df[storage_df['Table'] == DIM_CUSTOMERS_TABLE]\n",
    "axes[1].bar(customer_data['Format'], customer_data['Size_MB'], color=['#e74c3c', '#3498db', '#2ecc71'])\n",
    "axes[1].set_title('dim_customers - Storage Size Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Size (MB)', fontsize=12)\n",
    "axes[1].set_xlabel('Format', fontsize=12)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(customer_data['Size_MB']):\n",
    "    axes[1].text(i, v + 0.2, f'{v:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'storage_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Plot saved to: {PLOTS_DIR / 'storage_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 1: Full Scan Read Performance\n",
    "\n",
    "Measure how long it takes to read and count all records from each format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CSV read performance\n",
    "with BenchmarkTimer(\n",
    "    \"Read CSV - fact_sales (Full Scan)\",\n",
    "    description=\"Load and count all records from CSV\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    ") as timer:\n",
    "    csv_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\n",
    "        str(get_data_path(\"csv\", FACT_SALES_TABLE))\n",
    "    )\n",
    "    count = csv_df.count()\n",
    "    print(f\"Records: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Parquet read performance\n",
    "with BenchmarkTimer(\n",
    "    \"Read Parquet - fact_sales (Full Scan)\",\n",
    "    description=\"Load and count all records from Parquet\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    ") as timer:\n",
    "    parquet_df = spark.read.parquet(str(get_data_path(\"parquet\", FACT_SALES_TABLE)))\n",
    "    count = parquet_df.count()\n",
    "    print(f\"Records: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Delta read performance\n",
    "with BenchmarkTimer(\n",
    "    \"Read Delta - fact_sales (Full Scan)\",\n",
    "    description=\"Load and count all records from Delta\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    ") as timer:\n",
    "    delta_df = spark.read.format(\"delta\").load(str(get_data_path(\"delta\", FACT_SALES_TABLE)))\n",
    "    count = delta_df.count()\n",
    "    print(f\"Records: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 2: Columnar Aggregation Performance\n",
    "\n",
    "Test aggregation performance to highlight the advantage of columnar formats.\n",
    "We'll aggregate sales by product category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV aggregation\n",
    "with BenchmarkTimer(\n",
    "    \"CSV - Aggregation by Category\",\n",
    "    description=\"GroupBy product_category and sum amount\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    "):\n",
    "    csv_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\n",
    "        str(get_data_path(\"csv\", FACT_SALES_TABLE))\n",
    "    )\n",
    "    result = csv_df.groupBy(\"product_category\").agg(\n",
    "        F.sum(\"amount\").alias(\"total_amount\"),\n",
    "        F.count(\"*\").alias(\"num_transactions\")\n",
    "    ).orderBy(\"product_category\").collect()\n",
    "    print(f\"Categories: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet aggregation\n",
    "with BenchmarkTimer(\n",
    "    \"Parquet - Aggregation by Category\",\n",
    "    description=\"GroupBy product_category and sum amount\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    "):\n",
    "    parquet_df = spark.read.parquet(str(get_data_path(\"parquet\", FACT_SALES_TABLE)))\n",
    "    result = parquet_df.groupBy(\"product_category\").agg(\n",
    "        F.sum(\"amount\").alias(\"total_amount\"),\n",
    "        F.count(\"*\").alias(\"num_transactions\")\n",
    "    ).orderBy(\"product_category\").collect()\n",
    "    print(f\"Categories: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta aggregation\n",
    "with BenchmarkTimer(\n",
    "    \"Delta - Aggregation by Category\",\n",
    "    description=\"GroupBy product_category and sum amount\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    "):\n",
    "    delta_df = spark.read.format(\"delta\").load(str(get_data_path(\"delta\", FACT_SALES_TABLE)))\n",
    "    result = delta_df.groupBy(\"product_category\").agg(\n",
    "        F.sum(\"amount\").alias(\"total_amount\"),\n",
    "        F.count(\"*\").alias(\"num_transactions\")\n",
    "    ).orderBy(\"product_category\").collect()\n",
    "    print(f\"Categories: {len(result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 3: Filter Pushdown (Predicate Pushdown)\n",
    "\n",
    "Test how well each format supports predicate pushdown optimization.\n",
    "We'll filter for specific product categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV with filter\n",
    "with BenchmarkTimer(\n",
    "    \"CSV - Filter Electronics Category\",\n",
    "    description=\"Filter product_category = 'Electronics' and count\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    "):\n",
    "    csv_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\n",
    "        str(get_data_path(\"csv\", FACT_SALES_TABLE))\n",
    "    )\n",
    "    filtered = csv_df.filter(F.col(\"product_category\") == \"Electronics\")\n",
    "    count = filtered.count()\n",
    "    print(f\"Filtered records: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet with filter (should benefit from predicate pushdown)\n",
    "with BenchmarkTimer(\n",
    "    \"Parquet - Filter Electronics Category\",\n",
    "    description=\"Filter product_category = 'Electronics' and count\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    "):\n",
    "    parquet_df = spark.read.parquet(str(get_data_path(\"parquet\", FACT_SALES_TABLE)))\n",
    "    filtered = parquet_df.filter(F.col(\"product_category\") == \"Electronics\")\n",
    "    count = filtered.count()\n",
    "    print(f\"Filtered records: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta with filter (should benefit from data skipping)\n",
    "with BenchmarkTimer(\n",
    "    \"Delta - Filter Electronics Category\",\n",
    "    description=\"Filter product_category = 'Electronics' and count\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    "):\n",
    "    delta_df = spark.read.format(\"delta\").load(str(get_data_path(\"delta\", FACT_SALES_TABLE)))\n",
    "    filtered = delta_df.filter(F.col(\"product_category\") == \"Electronics\")\n",
    "    count = filtered.count()\n",
    "    print(f\"Filtered records: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 4: Selective Column Read\n",
    "\n",
    "Test columnar format advantage when reading only specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV - Read only 2 columns\n",
    "with BenchmarkTimer(\n",
    "    \"CSV - Select 2 Columns\",\n",
    "    description=\"Read only customer_id and amount columns\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    "):\n",
    "    csv_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\n",
    "        str(get_data_path(\"csv\", FACT_SALES_TABLE))\n",
    "    )\n",
    "    result = csv_df.select(\"customer_id\", \"amount\").count()\n",
    "    print(f\"Records: {result:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet - Read only 2 columns (should be much faster)\n",
    "with BenchmarkTimer(\n",
    "    \"Parquet - Select 2 Columns\",\n",
    "    description=\"Read only customer_id and amount columns\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    "):\n",
    "    parquet_df = spark.read.parquet(str(get_data_path(\"parquet\", FACT_SALES_TABLE)))\n",
    "    result = parquet_df.select(\"customer_id\", \"amount\").count()\n",
    "    print(f\"Records: {result:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta - Read only 2 columns (should also benefit from columnar format)\n",
    "with BenchmarkTimer(\n",
    "    \"Delta - Select 2 Columns\",\n",
    "    description=\"Read only customer_id and amount columns\",\n",
    "    spark=spark,\n",
    "    clear_cache=True\n",
    "):\n",
    "    delta_df = spark.read.format(\"delta\").load(str(get_data_path(\"delta\", FACT_SALES_TABLE)))\n",
    "    result = delta_df.select(\"customer_id\", \"amount\").count()\n",
    "    print(f\"Records: {result:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze benchmark results\n",
    "from config import BENCHMARK_LOG_FILE\n",
    "import csv\n",
    "\n",
    "# Read the log file\n",
    "benchmark_results = []\n",
    "with open(BENCHMARK_LOG_FILE, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        if 'fact_sales' in row['test_name'] and row['status'] == 'SUCCESS':\n",
    "            benchmark_results.append(row)\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "results_df = pd.DataFrame(benchmark_results)\n",
    "results_df['duration_seconds'] = results_df['duration_seconds'].astype(float)\n",
    "\n",
    "# Display recent benchmarks\n",
    "print(\"\\nRecent Benchmark Results:\")\n",
    "print(results_df[['test_name', 'duration_seconds']].tail(12).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison visualization\n",
    "# Extract format from test name and categorize benchmarks\n",
    "def extract_format_and_test(test_name):\n",
    "    if 'CSV' in test_name:\n",
    "        fmt = 'CSV'\n",
    "    elif 'Parquet' in test_name:\n",
    "        fmt = 'Parquet'\n",
    "    elif 'Delta' in test_name:\n",
    "        fmt = 'Delta'\n",
    "    else:\n",
    "        fmt = 'Unknown'\n",
    "    \n",
    "    if 'Full Scan' in test_name:\n",
    "        test = 'Full Scan'\n",
    "    elif 'Aggregation' in test_name:\n",
    "        test = 'Aggregation'\n",
    "    elif 'Filter' in test_name:\n",
    "        test = 'Filter'\n",
    "    elif 'Select 2 Columns' in test_name:\n",
    "        test = 'Column Select'\n",
    "    else:\n",
    "        test = 'Other'\n",
    "    \n",
    "    return fmt, test\n",
    "\n",
    "# Only use the most recent format benchmarks (last 12 entries)\n",
    "recent_results = results_df.tail(12).copy()\n",
    "recent_results[['format', 'test_type']] = recent_results['test_name'].apply(\n",
    "    lambda x: pd.Series(extract_format_and_test(x))\n",
    ")\n",
    "\n",
    "# Filter for comparison tests\n",
    "comparison_tests = recent_results[recent_results['test_type'].isin(['Full Scan', 'Aggregation', 'Filter', 'Column Select'])]\n",
    "\n",
    "if len(comparison_tests) > 0:\n",
    "    # Create grouped bar chart\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Pivot data for grouped bars\n",
    "    pivot_data = comparison_tests.pivot_table(\n",
    "        index='test_type',\n",
    "        columns='format',\n",
    "        values='duration_seconds',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    \n",
    "    # Plot grouped bars\n",
    "    pivot_data.plot(kind='bar', ax=ax, color=['#e74c3c', '#3498db', '#2ecc71'], width=0.7)\n",
    "    \n",
    "    ax.set_title('Format Performance Comparison - fact_sales', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylabel('Duration (seconds)', fontsize=12)\n",
    "    ax.set_xlabel('Benchmark Type', fontsize=12)\n",
    "    ax.legend(title='Format', fontsize=11)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / 'format_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Plot saved to: {PLOTS_DIR / 'format_performance_comparison.png'}\")\n",
    "else:\n",
    "    print(\"⚠ Not enough data for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive benchmark summary\n",
    "print_benchmark_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "Expected Performance Characteristics:\n",
    "\n",
    "1. STORAGE EFFICIENCY:\n",
    "   - Parquet/Delta: 50-70% smaller than CSV due to compression\n",
    "   - Columnar format benefits: Better compression ratios\n",
    "\n",
    "2. FULL SCAN PERFORMANCE:\n",
    "   - CSV: Slowest (row-oriented, no compression)\n",
    "   - Parquet/Delta: Faster (columnar, compressed)\n",
    "\n",
    "3. AGGREGATION PERFORMANCE:\n",
    "   - Columnar formats excel: Only read required columns\n",
    "   - CSV must read all columns regardless\n",
    "\n",
    "4. FILTER PUSHDOWN:\n",
    "   - Parquet/Delta: Support predicate pushdown\n",
    "   - Delta: Additional data skipping with statistics\n",
    "\n",
    "5. COLUMN PROJECTION:\n",
    "   - Parquet/Delta: Huge advantage when selecting few columns\n",
    "   - CSV: Must read entire row even for single column\n",
    "\"\"\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n✓ Format benchmark completed!\")\n",
    "print(\"Next step: Run notebook 03_join_optimization.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
